# Generated by Django 4.2.7 on 2025-09-20 18:08

from django.db import migrations, models
import django.utils.timezone


class Migration(migrations.Migration):

    dependencies = [
        ("scraper", "0002_pagesummary_documentembedding_and_more"),
    ]

    operations = [
        migrations.CreateModel(
            name="RobotsInfo",
            fields=[
                (
                    "id",
                    models.BigAutoField(
                        auto_created=True,
                        primary_key=True,
                        serialize=False,
                        verbose_name="ID",
                    ),
                ),
                ("created_at", models.DateTimeField(auto_now_add=True)),
                ("updated_at", models.DateTimeField(auto_now=True)),
                (
                    "domain",
                    models.CharField(db_index=True, max_length=255, unique=True),
                ),
                (
                    "robots_txt_content",
                    models.TextField(blank=True, help_text="Full robots.txt content"),
                ),
                (
                    "crawl_delay",
                    models.FloatField(
                        blank=True, help_text="Crawl delay in seconds", null=True
                    ),
                ),
                (
                    "request_rate_requests",
                    models.PositiveIntegerField(blank=True, null=True),
                ),
                (
                    "request_rate_seconds",
                    models.PositiveIntegerField(blank=True, null=True),
                ),
                ("preferred_host", models.CharField(blank=True, max_length=255)),
                (
                    "is_accessible",
                    models.BooleanField(
                        default=True, help_text="Whether robots.txt was accessible"
                    ),
                ),
                ("last_checked", models.DateTimeField(auto_now=True)),
                ("error_message", models.TextField(blank=True)),
            ],
            options={
                "verbose_name": "Robots.txt Info",
                "verbose_name_plural": "Robots.txt Info",
                "ordering": ["-last_checked"],
            },
        ),
        migrations.RemoveIndex(
            model_name="documentembedding",
            name="scraper_doc_embeddi_3385a7_idx",
        ),
        migrations.RemoveIndex(
            model_name="documentembedding",
            name="scraper_doc_model_u_11dc9c_idx",
        ),
        migrations.RemoveIndex(
            model_name="documentembedding",
            name="scraper_doc_created_2a975b_idx",
        ),
        migrations.RemoveField(
            model_name="documentembedding",
            name="embedding_id",
        ),
        migrations.RemoveField(
            model_name="documentembedding",
            name="embedding_vector",
        ),
        migrations.RemoveField(
            model_name="documentembedding",
            name="model_used",
        ),
        migrations.RemoveField(
            model_name="documentembedding",
            name="vector_dimensions",
        ),
        migrations.AddField(
            model_name="documentembedding",
            name="embedding_dimensions",
            field=models.PositiveIntegerField(default=3072),
        ),
        migrations.AddField(
            model_name="documentembedding",
            name="embedding_model",
            field=models.CharField(default="text-embedding-large-3", max_length=100),
        ),
        migrations.AddField(
            model_name="documentembedding",
            name="milvus_id",
            field=models.CharField(
                default=django.utils.timezone.now,
                help_text="ID in Milvus vector database",
                max_length=255,
                unique=True,
            ),
            preserve_default=False,
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="crawl_delay_used",
            field=models.FloatField(
                blank=True, help_text="Crawl delay applied in seconds", null=True
            ),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="crawl_depth",
            field=models.PositiveIntegerField(
                default=0, help_text="Depth level in recursive crawl"
            ),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="crawl_status",
            field=models.CharField(
                choices=[
                    ("success", "Success"),
                    ("failed", "Failed"),
                    ("robots_blocked", "Robots Blocked"),
                    ("invalid_url", "Invalid URL"),
                ],
                default="success",
                max_length=20,
            ),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="file_size",
            field=models.PositiveIntegerField(
                default=0, help_text="File size in bytes"
            ),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="html_content",
            field=models.TextField(blank=True, help_text="Raw HTML content"),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="meta_description",
            field=models.TextField(blank=True),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="meta_keywords",
            field=models.TextField(blank=True),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="parent_url",
            field=models.URLField(
                blank=True, help_text="URL that linked to this page", max_length=2000
            ),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="processing_time",
            field=models.FloatField(
                default=0.0, help_text="Time taken to process this page in seconds"
            ),
        ),
        migrations.AddField(
            model_name="scrapedpage",
            name="robots_allowed",
            field=models.BooleanField(
                default=True, help_text="Whether robots.txt allows crawling this URL"
            ),
        ),
        migrations.AlterField(
            model_name="apiendpoint",
            name="method",
            field=models.CharField(
                choices=[
                    ("GET", "GET"),
                    ("POST", "POST"),
                    ("PUT", "PUT"),
                    ("PATCH", "PATCH"),
                ],
                default="POST",
                max_length=10,
            ),
        ),
        migrations.AlterField(
            model_name="scrapedpage",
            name="raw_html",
            field=models.TextField(blank=True, help_text="Raw HTML content (legacy)"),
        ),
        migrations.AlterField(
            model_name="scrapedpage",
            name="status",
            field=models.CharField(
                choices=[
                    ("success", "Success"),
                    ("failed", "Failed"),
                    ("skipped", "Skipped"),
                    ("duplicate", "Duplicate"),
                    ("robots_blocked", "Robots Blocked"),
                    ("invalid_url", "Invalid URL"),
                ],
                default="success",
                max_length=20,
            ),
        ),
        migrations.AddIndex(
            model_name="documentembedding",
            index=models.Index(
                fields=["embedding_model"], name="scraper_doc_embeddi_4ef848_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="documentembedding",
            index=models.Index(
                fields=["milvus_id"], name="scraper_doc_milvus__0e24b5_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="scrapedpage",
            index=models.Index(
                fields=["crawl_status"], name="scraper_scr_crawl_s_f7d52d_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="scrapedpage",
            index=models.Index(
                fields=["crawl_depth"], name="scraper_scr_crawl_d_41a55a_idx"
            ),
        ),
        migrations.AddIndex(
            model_name="scrapedpage",
            index=models.Index(
                fields=["robots_allowed"], name="scraper_scr_robots__331678_idx"
            ),
        ),
    ]
